

import streamlit as st
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
import requests
import json

st.title("üê¶ Mini Project: Analisis Sentimen Tweet Indonesia")
st.write("Lengkapi kode untuk membuat aplikasi analisis sentimen tweet!")

# Load model IndoBERT
@st.cache_resource
def load_indobert():
    model_name = "indolem/indobert-base-uncased"  
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    

    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, 
        num_labels= 3 
    )
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    
    return tokenizer, model, device

# TODO
LABEL_MAPPING = {
    0: 'negatif',  
    1: 'netral',  
    2: 'positif'   
}

# TODO 3: Fungsi untuk analisis sentimen
def analyze_sentiment(texts):
    tokenizer, model, device = load_indobert()
    results = []
    
    for text in texts:
        # LENGKAPI KODE DI SINI
        # 1. Tokenize text (max_length=128)
        inputs = tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True,
            padding=True,
            max_length=128  # Berapa max length untuk tweet?
        )
        
        # 2. Pindahkan ke device yang tepat
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # 3. Prediksi sentimen
        with torch.no_grad():
            outputs = model(**inputs)
            probs = F.softmax(outputs.logits, dim=-1)
            predicted_label = torch.argmax(probs, dim=-1).item()
            confidence = probs[0][predicted_label].item()
        
        # 4. Map label ke sentimen
        sentiment = LABEL_MAPPING[predicted_label]  # Gunakan predicted_label
        
        results.append({
            'text': text,
            'sentiment': sentiment,
            'confidence': confidence,
            'prob_negatif': probs[0][0].item(),
            'prob_netral': probs[0][1].item(),
            'prob_positif': probs[0][2].item()
        })
    
    return pd.DataFrame(results)

# TODO 4: Fungsi evaluasi dengan OLLAMA (evaluasi keseluruhan)
def evaluate_with_ollama(df_results, model="llama3.2"):
    # Hitung statistik dataset
    total_tweets = len(df_results)
    sentiment_counts = df_results['sentiment'].value_counts()
    
    # LENGKAPI TEMPLATE PROMPT
    prompt_template = f"""
    Evaluasi hasil analisis sentimen Twitter Indonesia:
    
    Total tweets yang dianalisis: {total_tweets}
    
    Distribusi sentimen:
    [0  - negatif]
    [1  - netral]
    [2  - positif]
    
    Contoh hasil klasifikasi:
    [Baru coba sabun wajah ini, wanginya enak banget dan bikin kulit halus! - positif]
    [Packing oke, pengiriman cepat, barang sesuai deskripsi. Lumayan lah. - netral]
    [Produk ini gak sesuai ekspektasi, teksturnya aneh dan bikin lengket. - negatif]
    [Suka banget sama kualitas bahannya, adem dan jahitannya rapi! - positif]
    [Udah coba seminggu, belum kelihatan hasil yang signifikan sih. - netral]
    [Kecewa banget, baru dipakai 2 hari udah rusak. Gak recommended. - negatif]

    
    Pertanyaan evaluasi:
    1. Apakah distribusi sentimen masuk akal untuk data Twitter Indonesia?
    2. Apakah klasifikasi sudah akurat untuk bahasa gaul/informal?
    3. Berikan skor kualitas analisis (1-10) dan alasannya
    4. Apa saran perbaikan untuk model ini?
    """
    
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": prompt_template,
                "stream": False
            }
        )
        return response.json()['response']
    except Exception as e:
        return f"Error: {str(e)}"

# Helper function untuk membaca CSV
def read_csv_flexible(uploaded_file):
    for delimiter in [',', ';', '\t']:
        for encoding in ['utf-8', 'latin1', 'ISO-8859-1']:
            try:
                return pd.read_csv(uploaded_file, delimiter=delimiter, encoding=encoding)
            except:
                continue
    return pd.DataFrame()


# Interface Streamlit
tab1, tab2, tab3 = st.tabs(["üìù Data Input", "üîç Analisis", "üìä Evaluasi"])

with tab1:
    st.header("Input Data Tweet")
    
    # TODO 5: Buat dataframe dengan contoh tweet Indonesia
    # Minimal 10 tweet dengan berbagai sentimen
    sample_data = pd.DataFrame({
    'text': [
        "Baru coba sabun wajah ini, wanginya enak banget dan bikin kulit halus!",  # positif
        "Packing oke, pengiriman cepat, barang sesuai deskripsi. Lumayan lah.",    # netral
        "Produk ini gak sesuai ekspektasi, teksturnya aneh dan bikin lengket.",     # negatif
        "Suka banget sama kualitas bahannya, adem dan jahitannya rapi!",            # positif
        "Udah coba seminggu, belum kelihatan hasil yang signifikan sih.",           # netral
        "Kecewa banget, baru dipakai 2 hari udah rusak. Gak recommended.",          # negatif
        "Design produknya keren, cocok buat daily use. Value for money!",           # positif
        "Barang datang lengkap, tapi belum sempat dicoba. Semoga bagus.",           # netral
        "Serum ini bikin breakout parah, nyesel banget beli!",                      # negatif
        "Teknologi smart-nya beneran membantu. Simpel & efektif banget dipakai!"    # positif
    ]
})


    
    # Pilihan input data
    data_source = st.radio("Sumber data:", ["Sample Data", "Upload CSV"])
    
    if data_source == "Sample Data":
        st.write("Data tweet untuk analisis:")
        df = st.data_editor(sample_data, num_rows="dynamic")
    else:
        uploaded_file = st.file_uploader("Upload CSV file", type=['csv'])
        if uploaded_file:
            # TODO: Implementasi pembacaan CSV yang robust
            try:
                df = pd.read_csv(uploaded_file)
                st.success("File berhasil dibaca!")
            except:
                st.error("Error membaca file. Implementasi read_csv_flexible() untuk handling yang lebih baik")
                df = pd.DataFrame()
        else:
            df = pd.DataFrame()
    
    # Display data
    if not df.empty:
        st.dataframe(df)
        st.info(f"Total tweets: {len(df)}")
    
    # Simpan ke session state
    st.session_state['data'] = df

with tab2:
    st.header("Analisis Sentimen Tweet")
    
    if 'data' in st.session_state:
        df = st.session_state['data']
        
        # TODO 6: Implementasi tombol analisis
        if st.button("Analisis Sentimen", type="primary"):
            with st.spinner("Menganalisis tweet..."):
                # LENGKAPI KODE DI SINI
                results = analyze_sentiment(df['Text Tweet'].tolist()) # Panggil dengan parameter yang tepat
                st.session_state['results'] = results
                
                # Tampilkan hasil
                st.dataframe(results)
                
                # TODO 7: Tambahkan visualisasi
                # Hint: Gunakan st.bar_chart untuk distribusi sentimen
                col1, col2 = st.columns(2)
                
                with col1:
                    # Visualisasi distribusi sentimen
                    st.subheader("Distribusi Sentimen")
                    st.bar_chart(results['sentiment'].value_counts())
                    pass
                
                with col2:
                    # Visualisasi rata-rata probabilitas
                    st.subheader("Rata-rata Probabilitas")
                    avg_probs = results[['prob_negatif', 'prob_netral', 'prob_positif']].mean()
                    st.bar_chart(avg_probs)
                    pass

with tab3:
    st.header("Evaluasi dengan OLLAMA")
    
    if 'results' in st.session_state:
        results = st.session_state['results']
        
        # Model selection
        ollama_model = st.selectbox(
            "Model OLLAMA:",
            ["llama3.2", "llama3.1"],
            index=0
        )
        
        # TODO 8: Implementasi evaluasi keseluruhan
        if st.button("Evaluasi Dataset"):
            with st.spinner("Evaluasi dengan OLLAMA..."):
                # LENGKAPI KODE DI SINI
                evaluation = evaluate_with_ollama(results, ollama_model) # Parameter yang tepat
                
                st.subheader("Hasil Evaluasi:")
                st.text_area("", evaluation, height=300)
        
        # TODO 9: Tambahkan fitur download hasil
        if st.button("Download Hasil CSV"):
            # LENGKAPI KODE DI SINI
            csv = results.to_csv(index=False)
            st.download_button(
                label="Download",
                data=csv, # Data CSV
                file_name="hasil_sentimen.csv",
                mime="text/csv"
            )

# TODO 10: Lengkapi sidebar dengan informasi
with st.sidebar:
    st.header("üìö Panduan Mini Project")
    st.write("""
    **Tugas Anda:**
    1. Load model IndoBERT
    2. Definisikan label mapping
    3. Implementasi analisis sentimen
    4. Buat evaluasi OLLAMA
    5. Siapkan data tweet Indonesia
    6. Implementasi analisis
    7. Tambahkan visualisasi
    8. Evaluasi dataset
    9. Fitur download
    10. Lengkapi dokumentasi
    
    **Model Info:**
    - Base: IndoBERT
    - Task: Tweet sentiment analysis
    - Classes: Positif, Netral, Negatif
    
    **Tips:**
    - Tweet max 128 tokens
    - Gunakan bahasa informal/gaul
    - Perhatikan confidence score
    """)
    
    # TODO 11: Tambahkan status check
    st.header("üîß Status")
    # Cek apakah model berhasil di-load
    # Cek apakah GPU tersedia